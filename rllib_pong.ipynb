{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show a rendom agent playing pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a custom Pong Environment\n",
    "\n",
    "Skipping initial frames to start without initial idle frames and Cutting episodes after 200 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "\n",
    "class CustomPongEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        super().__init__()\n",
    "        self.default_env = gym.make(\"ALE/Pong-v5\")\n",
    "\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 3), dtype=np.uint8)\n",
    "        self.action_space = self.default_env.action_space  # Example: move up or down\n",
    "        self.max_episode_steps = 200  # Set a max length for each episode to around 20 seconds of playtime\n",
    "        self.current_step = 0\n",
    "        self.skip_initial_steps = 40 \n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_step = 0\n",
    "        obs, info = self.default_env.reset()\n",
    "            # Skip the first `skip_initial_steps` by taking no-op actions\n",
    "        for _ in range(self.skip_initial_steps):\n",
    "            obs, _, terminated, truncated, _ = self.default_env.step(0)  # Assuming action `0` is no-op\n",
    "            if terminated or truncated:  # If the episode ends during skipping, reset again\n",
    "                obs, _ = self.default_env.reset(**kwargs)\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = self.default_env.step(action)\n",
    "        done = terminated or truncated or (self.current_step >= self.max_episode_steps)\n",
    "        return obs, reward, done, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show a random agent playing pong\n",
    "evaluate the episodes make sense and the agent acts while not getting stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode terminated\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import shimmy\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "\n",
    "import PIL.Image\n",
    "env_name = \"ALE/Pong-v5\"\n",
    "\n",
    "print(gym.pprint_registry())\n",
    "env = CustomPongEnv()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(observation.shape)\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "    if terminated:\n",
    "        observation, info = env.reset()\n",
    "        print(\"episode terminated\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a wrapper for the observation\n",
    "* convert to grayscale\n",
    "* normalize observation\n",
    "* convert data shape\n",
    "* removing top and bottom which is not part of the playing field\n",
    "* down sample the image for performance\n",
    "* adding padding around the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.preprocessors import Preprocessor\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class MyWrapper(gym.ObservationWrapper):\n",
    "    dim = (210,160)\n",
    "    padding = 2\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Adjust the observation space to reflect the grayscale image shape\n",
    "        obs_shape = self.observation_space.shape\n",
    "        print(type(env.observation_space))\n",
    "        env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(84, 84, 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        print(type(self.observation_space))\n",
    "\n",
    "    def observation(self, incomming_observation):\n",
    "        observation = np.copy(incomming_observation)\n",
    "        # Convert the observation to grayscale\n",
    "        observation = np.dot(observation[...,:3], [0.333, 0.333, 0.333])\n",
    "        # Normalize the grayscale image to the range [0, 1]\n",
    "        observation = observation / 255.0\n",
    "        observation = observation[..., np.newaxis]\n",
    "        # Remove all but playingfield\n",
    "        observation = observation[34:-16]\n",
    "        # Reduce size of the image for faster inference\n",
    "        observation = observation[::2, ::2]\n",
    "        observation = np.pad(observation, ((self.padding, self.padding), (self.padding, self.padding), (0, 0)),mode='constant', constant_values=0) \n",
    "        return observation\n",
    "# Register the preprocessor\n",
    "#ModelCatalog.register_custom_preprocessor(\"custom_obs_preprocessor\", CustomObservationPreprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for debugging purposes\n",
    "visualization of a episode in mp4 video format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_observations_as_video(observations, output_path, fps=30):\n",
    "    \"\"\"\n",
    "    Saves a sequence of observations (frames) as a video file, with numerical array overlays.\n",
    "\n",
    "    Args:\n",
    "        observations (torch.Tensor or list of torch.Tensor): Tensor of frames with shape [batch_size, H, W, C]\n",
    "                                                             or a list of tensors each shaped [1, H, W, 1].\n",
    "        output_path (str): Path to save the output video file.\n",
    "        fps (int): Frames per second for the output video.\n",
    "    \"\"\"\n",
    "    # Handle PyTorch tensor input\n",
    "    if isinstance(observations, torch.Tensor):\n",
    "        # Ensure observations have shape [batch_size, H, W, C]\n",
    "        if len(observations.shape) == 4:  # [batch_size, H, W, C]\n",
    "            observations = [obs.squeeze().cpu().numpy() for obs in observations]\n",
    "        else:\n",
    "            raise ValueError(\"Expected observations tensor with shape [batch_size, H, W, C].\")\n",
    "\n",
    "    # Ensure observations is not empty\n",
    "    if observations is None or len(observations) == 0:\n",
    "        print(\"No observations to save.\")\n",
    "        return\n",
    "\n",
    "    # Process each observation\n",
    "    processed_frames = []\n",
    "    for obs in observations:\n",
    "        frame_rgb = None\n",
    "        # If observation is grayscale, convert to RGB\n",
    "        if len(obs.shape) == 2:  # Shape: [H, W]\n",
    "            frame_rgb = cv2.cvtColor((obs * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        elif len(obs.shape) == 3 and obs.shape[-1] == 1:  # Shape: [H, W, 1]\n",
    "            frame_rgb = cv2.cvtColor((obs.squeeze(-1) * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        elif len(obs.shape) == 3 and obs.shape[-1] == 3:  # Shape: [H, W, 3]\n",
    "            frame_rgb = (obs * 255).astype(np.uint8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected observation shape: {obs.shape}\")\n",
    "\n",
    "        # Overlay numerical values as text on the frame\n",
    "        overlay_frame = frame_rgb.copy()\n",
    "        height, width = frame_rgb.shape[:2]\n",
    "\n",
    "        processed_frames.append(overlay_frame)\n",
    "\n",
    "    # Get video dimensions\n",
    "    height, width, _ = processed_frames[0].shape\n",
    "\n",
    "    # Define the codec and initialize VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in processed_frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Distribution for Epsilon greedy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ray.rllib.models.distributions import Distribution\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.rllib.models.torch.torch_distributions import (TorchDistribution, TorchDeterministic)\n",
    "\n",
    "class CustomDistribution(TorchDistribution):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        logits: torch.Tensor = None,\n",
    "        probs: torch.Tensor = None,\n",
    "    ) -> None:\n",
    "        # We assert this here because to_deterministic makes this assumption.\n",
    "        assert (probs is None) != (\n",
    "            logits is None\n",
    "        ), \"Exactly one out of `probs` and `logits` must be set!\"\n",
    "\n",
    "        self.probs = probs\n",
    "        self.logits = logits\n",
    "        super().__init__(logits=logits, probs=probs)\n",
    "\n",
    "        # Build this distribution only if really needed (in `self.rsample()`). It's\n",
    "        # quite expensive according to cProfile.\n",
    "        self._one_hot = None\n",
    "        \n",
    "        # Epsilon settings\n",
    "        self.epsilon = 1\n",
    "        self.final_epsilon = 0.01 # 1 % of actions shall remain random\n",
    "        self.epsilon_decay = 5e-5  # Adjust decay rate as needed -> 10k Steps before min randomeness is reached\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        \"\"\"\n",
    "        Apply epsilon-greedy sampling logic.\n",
    "        \"\"\"\n",
    "        # Use torch's Categorical to handle logits or probs\n",
    "        categorical_dist = self._get_torch_distribution(\n",
    "            logits=self.logits, probs=self.probs\n",
    "        )\n",
    "\n",
    "        self.epsilon = max(\n",
    "                self.final_epsilon,\n",
    "                self.epsilon - self.epsilon_decay,\n",
    "            )\n",
    "        \n",
    "        # Get greedy action (exploitation)\n",
    "        greedy_action = torch.argmax(self.logits if self.logits is not None else self.probs, dim=-1)\n",
    "\n",
    "        # Get random action (exploration)\n",
    "        random_action = torch.randint(0, self.logits.shape[-1], greedy_action.shape)\n",
    "\n",
    "        # Perform epsilon-greedy decision\n",
    "        exploration_mask = torch.rand(greedy_action.shape) < self.epsilon\n",
    "        final_action = torch.where(exploration_mask, random_action, greedy_action)\n",
    "\n",
    "        return final_action\n",
    "\n",
    "    def _get_torch_distribution(\n",
    "        self,\n",
    "        logits: torch.Tensor = None,\n",
    "        probs: torch.Tensor = None,\n",
    "    ) -> \"torch.distributions.Distribution\":\n",
    "        return torch.distributions.categorical.Categorical(logits=logits, probs=probs)\n",
    "\n",
    "    def required_input_dim(space: gym.Space, **kwargs) -> int:\n",
    "        assert isinstance(space, gym.spaces.Discrete)\n",
    "        return int(space.n)\n",
    "\n",
    "    def rsample(self, sample_shape=()):\n",
    "        return self.sample(sample_shape=sample_shape)\n",
    "\n",
    "    @classmethod\n",
    "    def from_logits(cls, logits, **kwargs) -> \"CustomDistribution\":\n",
    "        return CustomDistribution(logits=logits, **kwargs)\n",
    "\n",
    "    def to_deterministic(self) -> \"TorchDeterministic\":\n",
    "        if self.probs is not None:\n",
    "            probs_or_logits = self.probs\n",
    "        else:\n",
    "            probs_or_logits = self.logits\n",
    "\n",
    "        return CustomDistribution(loc=torch.argmax(probs_or_logits, dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a DQN Model\n",
    "* Implementation of an online model\n",
    "* Implemenation of a target network with update policy\n",
    "* Implementation of forward inference\n",
    "* returning logits for the current and next observation as well as logits for the training network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleConfig\n",
    "from typing import Any, Dict\n",
    "\n",
    "from typing import Any, Dict, Type\n",
    "from ray.rllib.models.torch.torch_distributions import TorchDistribution\n",
    "\n",
    "from torch import flatten\n",
    "import json\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class CustomAtariRLModule(TorchRLModule):\n",
    "    def setup(self):\n",
    "        # Extract observation and action space information from the config\n",
    "        obs_shape = self.observation_space.shape  # (1, 84, 84)\n",
    "        num_actions = self.action_space.n        # Number of discrete actions (e.g., 6)\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Compute the size of the flattened layer\n",
    "        flattened_size = 64 * 7 * 7  # Adjusted for (84, 84) input size\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.action_head = nn.Linear(512, num_actions)\n",
    "        \n",
    "        # Target network\n",
    "        self.target_conv1 = nn.Conv2d(1, 32, 8, stride=4)\n",
    "        self.target_conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.target_conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        self.target_fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.target_action_head = nn.Linear(512, self.action_space.n)\n",
    "\n",
    "        # Copy weights from the online network to the target network\n",
    "        self._update_target_network()\n",
    "\n",
    "        self.target_update_frequency = 400;\n",
    "        self.iter = 0;\n",
    "\n",
    "    def get_exploration_action_dist_cls(self) -> Type[TorchDistribution]:\n",
    "        return CustomDistribution\n",
    "\n",
    "    def _forward_train(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        # Forward pass through the network\n",
    "        #print(batch[\"obs\"].size())\n",
    "        self.iter  += 1\n",
    "\n",
    "        #update targaet network\n",
    "        if self.iter % self.target_update_frequency == 0:\n",
    "            self._update_target_network()\n",
    "        \n",
    "        action_logits = None\n",
    "\n",
    "        #if batch[\"obs\"].shape[0] > 1:\n",
    "        #    print(\"batchsize: \", batch[\"obs\"].shape[0])\n",
    "        #    save_observations_as_video(batch[\"obs\"], \"./test_obs_vid.mp4\")\n",
    "\n",
    "        x = batch[\"obs\"].permute(0, 3, 1, 2)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        #x = flatten(x).reshape(1,64*7*7)\n",
    "        x = x.reshape(x.size(0), -1) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_logits = self.action_head(x)\n",
    "\n",
    "\n",
    "        #print(\"actionLogits: \", action_logits)\n",
    "\n",
    "        next_q_values = None\n",
    "\n",
    "        # Compute Q-values for next states using the same logic (or via a target network)\n",
    "        if 'new_obs' in batch.keys():\n",
    "            #print(\"next obs available\")\n",
    "            next_obs = batch[\"new_obs\"].permute(0, 3, 1, 2)  # Handle next states\n",
    "            next_x = torch.relu(self.conv1(next_obs))\n",
    "            next_x = torch.relu(self.conv2(next_x))\n",
    "            next_x = torch.relu(self.conv3(next_x))\n",
    "            next_x = next_x.reshape(next_x.size(0), -1)\n",
    "            next_x = torch.relu(self.fc1(next_x))\n",
    "            next_q_values = self.action_head(next_x)   # Q-values for next observations\n",
    "\n",
    "\n",
    "            # Compute Q-values for next observations using the target network\n",
    "            with torch.no_grad():  # Ensure no gradients are computed for the target network\n",
    "                target_next_x = torch.relu(self.target_conv1(next_obs))\n",
    "                target_next_x = torch.relu(self.target_conv2(target_next_x))\n",
    "                target_next_x = torch.relu(self.target_conv3(target_next_x))\n",
    "                target_next_x = target_next_x.reshape(target_next_x.size(0), -1)\n",
    "                target_next_x = torch.relu(self.target_fc1(target_next_x))\n",
    "                qf_target_next_preds = self.target_action_head(target_next_x)\n",
    "                \n",
    "                #print(\"targetActionLogits: \", qf_target_next_preds)\n",
    "            return {\"action_dist_inputs\": action_logits, \"qf_preds\":action_logits, \"qf_target_next_preds\": qf_target_next_preds, \"qf_next_preds\":next_q_values} \n",
    "        # Return action distribution\n",
    "        return {\"action_dist_inputs\": action_logits, \"qf_preds\":action_logits}\n",
    "\n",
    "    def _forward_inference(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            return self._forward_train(batch, explore = False)\n",
    "\n",
    "    def _forward_exploration(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        #with torch.no_grad():\n",
    "         return self._forward_train(batch)\n",
    "\n",
    "\n",
    "    def _update_target_network(self):\n",
    "        for target_param, param in zip(self.target_parameters(), self.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        #print(list(self.conv1.parameters()))\n",
    "\n",
    "    def target_parameters(self):\n",
    "    # Return all target network parameters\n",
    "        return list(self.target_conv1.parameters()) + \\\n",
    "               list(self.target_conv2.parameters()) + \\\n",
    "               list(self.target_conv3.parameters()) + \\\n",
    "               list(self.target_fc1.parameters()) + \\\n",
    "               list(self.target_action_head.parameters())\n",
    "\n",
    "    def parameters(self):\n",
    "        # Return all online network parameters\n",
    "        return list(self.conv1.parameters()) + \\\n",
    "               list(self.conv2.parameters()) + \\\n",
    "               list(self.conv3.parameters()) + \\\n",
    "               list(self.fc1.parameters()) + \\\n",
    "               list(self.action_head.parameters())\n",
    "\n",
    "# Register the custom RLModule\n",
    "ModelCatalog.register_custom_model(\"custom_atari_module\", CustomAtariRLModule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DQN Agent of Rllib with needed Settings and debug callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:29:57,719\tWARNING deprecation.py:50 -- DeprecationWarning: `rollouts` has been deprecated. Use `AlgorithmConfig.env_runners(..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CustomAtariRLModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 103\u001b[0m\n\u001b[1;32m     66\u001b[0m environment \u001b[38;5;241m=\u001b[39m custom_env_creator({})\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Configure the DQN Algorithm\u001b[39;00m\n\u001b[1;32m     69\u001b[0m config:DQNConfig \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m     DQNConfig()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#.callbacks(DebugCallback)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m.\u001b[39menvironment(\n\u001b[1;32m     73\u001b[0m         env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_pong_env\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Atari Pong environment (gymnasium)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         env_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m#env_config={\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m#    \"preprocessor_pref\": \"custom_obs_preprocessor\",\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m#},\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m#.reporting(min_train_timesteps_per_iteration=20,\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#           min_time_s_per_iteration=0,\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m#           min_sample_timesteps_per_iteration = 0,)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;241m.\u001b[39mframework(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# Use PyTorch (preferred for modern RL)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;241m.\u001b[39menv_runners(num_env_runners\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     84\u001b[0m         num_gpus_per_env_runner\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     85\u001b[0m         batch_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncate_episodes\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m.\u001b[39mrollouts(rollout_fragment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     87\u001b[0m               batch_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncate_episodes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(\n\u001b[1;32m     89\u001b[0m         replay_buffer_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     90\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrioritizedEpisodeReplayBuffer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapacity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     92\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     93\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     94\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     95\u001b[0m             },\n\u001b[1;32m     96\u001b[0m         train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     97\u001b[0m         gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m,            \u001b[38;5;66;03m# Discount factor\u001b[39;00m\n\u001b[1;32m     98\u001b[0m         lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,               \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m     99\u001b[0m         target_network_update_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,  \u001b[38;5;66;03m# Update target network frequency\u001b[39;00m\n\u001b[1;32m    100\u001b[0m         num_steps_sampled_before_learning_starts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;241m.\u001b[39mrl_module(\n\u001b[0;32m--> 103\u001b[0m         rl_module_spec\u001b[38;5;241m=\u001b[39mRLModuleSpec(module_class\u001b[38;5;241m=\u001b[39m\u001b[43mCustomAtariRLModule\u001b[49m)\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;241m.\u001b[39mresources(num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Set this to 1 if you have a GPU\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m.\u001b[39mapi_stack(enable_rl_module_and_learner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m         enable_env_runner_and_connector_v2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;241m.\u001b[39mdebugging(logger_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;241m.\u001b[39mevaluation(\n\u001b[1;32m    110\u001b[0m         evaluation_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    111\u001b[0m         evaluation_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplore\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;241m.\u001b[39mlearners(\n\u001b[1;32m    114\u001b[0m         num_learners\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    115\u001b[0m         num_gpus_per_learner\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_buffer_config\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomAtariRLModule' is not defined"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "import ale_py\n",
    "import shimmy\n",
    "from gymnasium.envs.registration import register\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "from ray.rllib.env.single_agent_env_runner import SingleAgentEnvRunner\n",
    "\n",
    "\n",
    "\n",
    "# Wrap the environment\n",
    "def custom_env_creator(env_config):\n",
    "    env = CustomPongEnv() #gym.make(\"ALE/Pong-v5\",  **env_config.get(\"gym_kwargs\", {}))\n",
    "    print(env.observation_space, env.action_space)\n",
    "    env = MyWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "class DebugCallback(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, episode, **kwargs):\n",
    "        print(f\"Episode started.\")\n",
    "    \n",
    "    def on_episode_end(self, *, episode, **kwargs):\n",
    "        print(f\"Episode ended with length {episode.__len__()}.\")\n",
    "    \n",
    "    def on_sample_end(self, *, samples, **kwargs):\n",
    "        print(\"Sample batch collected:\")\n",
    "    \n",
    "        # If the samples are in SAEps format (list of episodes):\n",
    "        if isinstance(samples, list):\n",
    "            for idx, episode in enumerate(samples):\n",
    "                save_observations_as_video(episode.get_observations(), \"./samles.mp4\")\n",
    "                print(f\"  Episode {idx}:\")\n",
    "                print(f\"    Length: {episode.__len__()}\")  # Total length of the episode\n",
    "                print(f\"    Done: {episode.is_done}\")  # Done flag of the episode\n",
    "                print(f\"    Reward: {episode.get_return}\")  # Episode reward\n",
    "                \n",
    "                # Access observations, rewards, and done flags\n",
    "                # Assuming `episode` stores these as numpy arrays or lists\n",
    "                print(f\"    Observations (sample size): {len(episode.get_observations())}\")  # Show first observation in the episode\n",
    "                #print(f\"    Rewards (sample): {episode.get_rewards[:5]}\")  # Show first 5 rewards in the episode\n",
    "                #print(f\"    Done flags (sample): {episode.dones[:5]}\")  # Show first 5 done flags in the episode\n",
    "    \n",
    "        else:\n",
    "            print(f\"  Unexpected format: {samples}\")\n",
    "\n",
    "        #save_observations_as_video(samples.get_observations(), \"./samles.mp4\")\n",
    "\n",
    "    def before_learn_on_batch(self, *, policy:Policy, train_batch:SampleBatch, result, **kwargs):\n",
    "        print(\"====== Training Batch Debug ======\")\n",
    "        print(f\"Train Batch Size: {train_batch.count}\")\n",
    "\n",
    "        if SampleBatch.SEQ_LENS in train_batch:\n",
    "            print(f\"Sequence Lengths: {train_batch[SampleBatch.SEQ_LENS]}\")\n",
    "        \n",
    "        print(\"Sampled Observations:\")\n",
    "        save_observations_as_video(train_batch[\"obs\"], \"./train_batch.mp4\")\n",
    "        \n",
    "        raise Exception(\"blub\")\n",
    "\n",
    "environment = custom_env_creator({})\n",
    "\n",
    "# Configure the DQN Algorithm\n",
    "config:DQNConfig = (\n",
    "    DQNConfig()\n",
    "    #.callbacks(DebugCallback)\n",
    "    .environment(\n",
    "        env=\"custom_pong_env\",  # Atari Pong environment (gymnasium)\n",
    "        env_config={\"episodic\": True},\n",
    "        #env_config={\n",
    "        #    \"preprocessor_pref\": \"custom_obs_preprocessor\",\n",
    "        #},\n",
    "    )\n",
    "    #.reporting(min_train_timesteps_per_iteration=20,\n",
    "    #           min_time_s_per_iteration=0,\n",
    "    #           min_sample_timesteps_per_iteration = 0,)\n",
    "    .framework(\"torch\")     # Use PyTorch (preferred for modern RL)\n",
    "    .env_runners(num_env_runners=1,\n",
    "        num_gpus_per_env_runner=1,\n",
    "        batch_mode='truncate_episodes') \n",
    "    .rollouts(rollout_fragment_length=200,\n",
    "              batch_mode='truncate_episodes')\n",
    "    .training(\n",
    "        replay_buffer_config={\n",
    "                \"type\": \"PrioritizedEpisodeReplayBuffer\",\n",
    "                \"capacity\": 2048,\n",
    "                \"alpha\": 0.5,\n",
    "                \"beta\": 0.5,\n",
    "                \"replay_sequence_length\":200\n",
    "            },\n",
    "        train_batch_size=200,\n",
    "        gamma=0.50,            # Discount factor\n",
    "        lr=1e-4,               # Learning rate\n",
    "        target_network_update_freq=200,  # Update target network frequency\n",
    "        num_steps_sampled_before_learning_starts = 1000,\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=RLModuleSpec(module_class=CustomAtariRLModule)\n",
    "    )\n",
    "    .resources(num_gpus=1)  # Set this to 1 if you have a GPU\n",
    "    .api_stack(enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,)\n",
    "    .debugging(logger_config={\"log_level\": \"DEBUG\"})\n",
    "    .evaluation(\n",
    "        evaluation_interval=5,\n",
    "        evaluation_config={\"explore\": False}\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=1,\n",
    "        num_gpus_per_learner=1,\n",
    "    )\n",
    "    #.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)\n",
    ")\n",
    "\n",
    "\n",
    "print(config[\"replay_buffer_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Resource Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Number of GPUs: 1\n",
      "CUDA current device: 0\n",
      "CUDA device name: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RAY_record_ref_creation_sites=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:27:17,904\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-02-26 18:27:18,642\tWARNING dqn.py:418 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      ":job_id:01000000\n",
      ":actor_name:SingleAgentEnvRunner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:569: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      ":job_id:01000000\n",
      ":actor_name:SingleAgentEnvRunner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:27:19,155\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "2025-02-26 18:27:19,512\tWARNING dqn.py:418 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:27:19,808\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:_WrappedExecutable\n",
      "2025-02-26 18:27:21,248\tINFO config.py:83 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "2025-02-26 18:27:21,351\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:_WrappedExecutable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:27:21,783\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "update from episode\n",
      "Iteration 0 results:\n",
      "date: 2025-02-26_18-27-27\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    default_agent: -5.733333333333333\n",
      "  episode_duration_sec_mean: 0.8653746492850284\n",
      "  episode_len_max: 200\n",
      "  episode_len_mean: 200.0\n",
      "  episode_len_min: 200\n",
      "  episode_return_max: -5.0\n",
      "  episode_return_mean: -5.733333333333333\n",
      "  episode_return_min: -6.0\n",
      "  module_episode_returns_mean:\n",
      "    default_policy: -5.733333333333333\n",
      "  num_agent_steps_sampled:\n",
      "    default_agent: 1000\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    default_agent: 1000\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_sampled_lifetime: 1000\n",
      "  num_episodes: 5\n",
      "  num_episodes_lifetime: 5\n",
      "  num_module_steps_sampled:\n",
      "    default_policy: 1000\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    default_policy: 1000\n",
      "  sample: 0.9120512649087112\n",
      "  time_between_sampling: 0.014400676001813894\n",
      "  weights_seq_no: 0.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 1\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: server-0815\n",
      "iterations_since_restore: 1\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector_timer: 0.031540044117718935\n",
      "    num_env_steps_trained: 200\n",
      "    num_env_steps_trained_lifetime: 200\n",
      "    num_module_steps_trained: 200\n",
      "    num_module_steps_trained_lifetime: 200\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 1681062\n",
      "  default_policy:\n",
      "    default_optimizer_learning_rate: 0.0001\n",
      "    gradients_default_optimizer_global_norm: 0.02104579471051693\n",
      "    module_train_batch_size_mean: 200\n",
      "    num_module_steps_trained: 200\n",
      "    num_module_steps_trained_lifetime: 200\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 1681062\n",
      "    qf_loss: 0.010371267795562744\n",
      "    qf_max: 0.05196988955140114\n",
      "    qf_mean: -0.009692518971860409\n",
      "    qf_min: -0.03682706132531166\n",
      "    td_error_mean: 0.06097019091248512\n",
      "    total_loss: 0.010371267795562744\n",
      "    weights_seq_no: 1.0\n",
      "node_ip: 192.168.178.28\n",
      "num_env_steps_sampled_lifetime: 1000\n",
      "num_training_step_calls_per_iteration: 5\n",
      "perf:\n",
      "  cpu_util_percent: 32.1375\n",
      "  ram_util_percent: 18.3375\n",
      "pid: 1816755\n",
      "time_since_restore: 5.640670299530029\n",
      "time_this_iter_s: 5.640670299530029\n",
      "time_total_s: 5.640670299530029\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.9210927799261178\n",
      "  learner_update_timer: 0.837490351870656\n",
      "  replay_buffer_add_data_timer: 0.004902916187955931\n",
      "  replay_buffer_sampling_timer: 0.053836600854992867\n",
      "  replay_buffer_update_prios_timer: 0.002565051894634962\n",
      "  restore_workers: 2.463291617222596e-05\n",
      "  synch_weights: 0.11312112491577864\n",
      "  training_iteration: 5.633219199255109\n",
      "  training_step: 0.9366475122333145\n",
      "timestamp: 1740594447\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/tmp/tmp1mbs6pte), metrics={'timers': {'training_iteration': 5.633219199255109, 'restore_workers': 2.463291617222596e-05, 'training_step': 0.9366475122333145, 'env_runner_sampling_timer': 0.9210927799261178, 'replay_buffer_add_data_timer': 0.004902916187955931, 'replay_buffer_sampling_timer': 0.053836600854992867, 'learner_update_timer': 0.837490351870656, 'replay_buffer_update_prios_timer': 0.002565051894634962, 'synch_weights': 0.11312112491577864}, 'env_runners': {'num_agent_steps_sampled': {'default_agent': 1000}, 'num_module_steps_sampled': {'default_policy': 1000}, 'num_module_steps_sampled_lifetime': {'default_policy': 1000}, 'num_episodes_lifetime': 5, 'num_env_steps_sampled_lifetime': 1000, 'module_episode_returns_mean': {'default_policy': -5.733333333333333}, 'num_episodes': 5, 'sample': 0.9120512649087112, 'episode_duration_sec_mean': 0.8653746492850284, 'episode_return_mean': -5.733333333333333, 'agent_episode_returns_mean': {'default_agent': -5.733333333333333}, 'episode_return_max': -5.0, 'num_env_steps_sampled': 1000, 'episode_return_min': -6.0, 'num_agent_steps_sampled_lifetime': {'default_agent': 1000}, 'weights_seq_no': 0.0, 'episode_len_min': 200, 'episode_len_mean': 200.0, 'episode_len_max': 200, 'time_between_sampling': 0.014400676001813894}, 'num_training_step_calls_per_iteration': 5, 'learners': {'default_policy': {'gradients_default_optimizer_global_norm': 0.02104579471051693, 'total_loss': 0.010371267795562744, 'weights_seq_no': 1.0, 'module_train_batch_size_mean': 200, 'qf_max': 0.05196988955140114, 'qf_min': -0.03682706132531166, 'num_module_steps_trained_lifetime': 200, 'num_non_trainable_parameters': 0, 'td_error_mean': 0.06097019091248512, 'default_optimizer_learning_rate': 0.0001, 'num_trainable_parameters': 1681062, 'qf_mean': -0.009692518971860409, 'num_module_steps_trained': 200, 'qf_loss': 0.010371267795562744}, '__all_modules__': {'num_trainable_parameters': 1681062, 'num_module_steps_trained': 200, 'num_env_steps_trained_lifetime': 200, 'num_env_steps_trained': 200, 'num_module_steps_trained_lifetime': 200, 'num_non_trainable_parameters': 0, 'learner_connector_timer': 0.031540044117718935}}, 'num_env_steps_sampled_lifetime': 1000, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-02-26_18-27-27', 'timestamp': 1740594447, 'time_this_iter_s': 5.640670299530029, 'time_total_s': 5.640670299530029, 'pid': 1816755, 'hostname': 'server-0815', 'node_ip': '192.168.178.28', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'custom_pong_env', 'env_config': {'episodic': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 1, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 1, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.5, 'lr': 0.0001, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 200, 'num_epochs': 1, 'minibatch_size': None, 'shuffle_batch_per_epoch': False, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f328bde20e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 5, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': {'log_level': 'DEBUG'}, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': RLModuleSpec(module_class=<class '__main__.CustomAtariRLModule'>, observation_space=None, action_space=None, inference_only=False, learner_only=False, model_config=None, catalog_class=None, load_state_path=None, model_config_dict=None), 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'epsilon': [(0, 1.0), (10000, 0.05)], 'target_network_update_freq': 200, 'num_steps_sampled_before_learning_starts': 1000, 'store_buffer_in_checkpoints': False, 'adam_epsilon': 1e-08, 'tau': 1.0, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'td_error_loss_fn': 'huber', 'categorical_distribution_temperature': 1.0, 'replay_buffer_config': {'type': 'PrioritizedEpisodeReplayBuffer', 'capacity': 2048, 'alpha': 0.5, 'beta': 0.5, 'replay_sequence_length': 200}, 'lr_schedule': None, 'class': <class 'ray.rllib.algorithms.dqn.dqn.DQNConfig'>, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 5.640670299530029, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(32.1375), 'ram_util_percent': np.float64(18.3375)}})\n",
      "update from episode\n",
      "update from episode\n",
      "update from episode\n",
      "update from episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:27:32,834\tERROR actor_manager.py:804 -- Ray error (\u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1816755, ip=192.168.178.28, actor_id=80e0be3cd612de67e2a9de8601000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f32a80789d0>)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 113, in <lambda>\n",
      "    else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 199, in sample\n",
      "    samples = self._sample(\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 309, in _sample\n",
      "    results = self._try_env_step(actions_for_env)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/env_runner.py\", line 145, in _try_env_step\n",
      "    results = self.env.step(actions)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 82, in step\n",
      "    observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 222, in step\n",
      "    ) = self.envs[i].step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/core.py\", line 322, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/core.py\", line 551, in step\n",
      "    return self.observation(observation), reward, terminated, truncated, info\n",
      "  File \"/tmp/ipykernel_1816755/1144362532.py\", line 26, in observation\n",
      "    observation = np.dot(observation[...,:3], [0.333, 0.333, 0.333])\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.), taking actor 1 out of service.\n",
      "2025-02-26 18:27:32,835\tERROR actor_manager.py:635 -- \u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1816755, ip=192.168.178.28, actor_id=80e0be3cd612de67e2a9de8601000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f32a80789d0>)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 113, in <lambda>\n",
      "    else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 199, in sample\n",
      "    samples = self._sample(\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 309, in _sample\n",
      "    results = self._try_env_step(actions_for_env)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/env_runner.py\", line 145, in _try_env_step\n",
      "    results = self.env.step(actions)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 82, in step\n",
      "    observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 222, in step\n",
      "    ) = self.envs[i].step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/core.py\", line 322, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/core.py\", line 551, in step\n",
      "    return self.observation(observation), reward, terminated, truncated, info\n",
      "  File \"/tmp/ipykernel_1816755/1144362532.py\", line 26, in observation\n",
      "    observation = np.dot(observation[...,:3], [0.333, 0.333, 0.333])\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.\n",
      "NoneType: None\n",
      "2025-02-26 18:27:32,835\tWARNING rollout_ops.py:124 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-02-26 18:27:33,001\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "2025-02-26 18:27:33,073\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 113, in <lambda>\n",
      "    else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 199, in sample\n",
      "    samples = self._sample(\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 275, in _sample\n",
      "    assert to_module is not None\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update from episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:28:03,914\tERROR actor_manager.py:804 -- Ray error (\u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1816755, ip=192.168.178.28, actor_id=80e0be3cd612de67e2a9de8601000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f32a80789d0>)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "    time.sleep(self.config.delay_between_env_runner_restarts_s)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.), taking actor 1 out of service.\n",
      "2025-02-26 18:28:03,915\tERROR actor_manager.py:635 -- \u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1816755, ip=192.168.178.28, actor_id=80e0be3cd612de67e2a9de8601000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f32a80789d0>)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "    time.sleep(self.config.delay_between_env_runner_restarts_s)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.\n",
      "NoneType: None\n",
      "2025-02-26 18:28:03,916\tWARNING rollout_ops.py:124 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-02-26 18:28:04,104\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "2025-02-26 18:28:04,174\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 113, in <lambda>\n",
      "    else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 199, in sample\n",
      "    samples = self._sample(\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 275, in _sample\n",
      "    assert to_module is not None\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update from episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:28:45,838\tERROR actor_manager.py:804 -- Ray error (\u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1816755, ip=192.168.178.28, actor_id=80e0be3cd612de67e2a9de8601000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f32a80789d0>)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "    time.sleep(self.config.delay_between_env_runner_restarts_s)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.), taking actor 1 out of service.\n",
      "2025-02-26 18:28:45,840\tERROR actor_manager.py:635 -- \u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1816755, ip=192.168.178.28, actor_id=80e0be3cd612de67e2a9de8601000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f32a80789d0>)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "    time.sleep(self.config.delay_between_env_runner_restarts_s)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.\n",
      "NoneType: None\n",
      "2025-02-26 18:28:45,841\tWARNING rollout_ops.py:124 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-02-26 18:28:46,001\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "2025-02-26 18:28:46,066\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 113, in <lambda>\n",
      "    else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 199, in sample\n",
      "    samples = self._sample(\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 275, in _sample\n",
      "    assert to_module is not None\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update from episode\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.utils.replay_buffers import PrioritizedEpisodeReplayBuffer\n",
    "\n",
    "%env RAY_record_ref_creation_sites=1\n",
    "\n",
    "ray.shutdown()  # Cleanly shutdown the current Ray session\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True, local_mode=True, object_store_memory=1e10, num_cpus=28)#num_gpus=1, logging_level=\"debug\"\n",
    "print(ray.get_gpu_ids())\n",
    "\n",
    "tune.register_env(\"custom_pong_env\", custom_env_creator)\n",
    "\n",
    "\n",
    "# Build the DQN Algorithm\n",
    "dqn: Algorithm = config.build()\n",
    "#dqn.restore_from_path(\"/tmp/tmpm9j1s9hd\")\n",
    "\n",
    "# Train the agent\n",
    "print(\"Starting training...\")\n",
    "for i in range(301):  # Increase the number of iterations for better performance\n",
    "    result = dqn.train()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i} results:\")\n",
    "        print(pretty_print(result))\n",
    "        #Save checkpoints periodically\n",
    "    \n",
    "        checkpoint = dqn.save()\n",
    "        print(f\"Checkpoint saved at {checkpoint}\")\n",
    "\n",
    "# Shut down Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the model manually if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 06:14:12,248\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to submit task to actor ActorID(268fc8295ccff32ebaa8c67801000000) due to b\"Can't find actor 268fc8295ccff32ebaa8c67801000000. It might be dead or it's from a different cluster\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_result \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m path_to_checkpoint \u001b[38;5;241m=\u001b[39m save_result\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mpath\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn Algorithm checkpoint has been created inside directory: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:488\u001b[0m, in \u001b[0;36mTrainable.save\u001b[0;34m(self, checkpoint_dir)\u001b[0m\n\u001b[1;32m    485\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m checkpoint_dir \u001b[38;5;129;01mor\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mmkdtemp()\n\u001b[1;32m    486\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 488\u001b[0m checkpoint_dict_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m checkpoint_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_class_trainable_checkpoint(\n\u001b[1;32m    490\u001b[0m     checkpoint_dir, checkpoint_dict_or_path\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Clean up the temporary directory, since it's already been\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# reported + persisted to storage. If no storage is set, the user is\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# running the Trainable locally and is responsible for cleaning\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# up the checkpoint directory themselves.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:2611\u001b[0m, in \u001b[0;36mAlgorithm.save_checkpoint\u001b[0;34m(self, checkpoint_dir)\u001b[0m\n\u001b[1;32m   2608\u001b[0m \u001b[38;5;66;03m# New API stack: Delegate to the `Checkpointable` implementation of\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;66;03m# `save_to_path()`.\u001b[39;00m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_rl_module_and_learner:\n\u001b[0;32m-> 2611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2613\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(checkpoint_dir)\n\u001b[1;32m   2615\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__getstate__()\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/checkpoints.py:290\u001b[0m, in \u001b[0;36mCheckpointable.save_to_path\u001b[0;34m(self, path, state, filesystem)\u001b[0m\n\u001b[1;32m    288\u001b[0m     comp_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mpop(comp_name)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     comp_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_name\u001b[49m\u001b[43m)\u001b[49m[comp_name]\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# By providing the `state` arg, we make sure that the component does not\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# have to call its own `get_state()` anymore, but uses what's provided\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# here.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m comp\u001b[38;5;241m.\u001b[39msave_to_path(comp_path, filesystem\u001b[38;5;241m=\u001b[39mfilesystem, state\u001b[38;5;241m=\u001b[39mcomp_state)\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:2735\u001b[0m, in \u001b[0;36mAlgorithm.get_state\u001b[0;34m(self, components, not_components, **kwargs)\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# Get LearnerGroup state (w/ RLModule).\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_component(COMPONENT_LEARNER_GROUP, components, not_components):\n\u001b[0;32m-> 2735\u001b[0m     state[COMPONENT_LEARNER_GROUP] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_subcomponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMPONENT_LEARNER_GROUP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnot_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_subcomponents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCOMPONENT_LEARNER_GROUP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_components\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;66;03m# Get entire MetricsLogger state.\u001b[39;00m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;66;03m# TODO (sven): Make `MetricsLogger` a Checkpointable.\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m state[COMPONENT_METRICS_LOGGER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/core/learner/learner_group.py:755\u001b[0m, in \u001b[0;36mLearnerGroup.get_state\u001b[0;34m(self, components, not_components, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         _comps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_subcomponents(COMPONENT_LEARNER, components)\n\u001b[1;32m    754\u001b[0m         _not_comps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_subcomponents(COMPONENT_LEARNER, not_components)\n\u001b[0;32m--> 755\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_comps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_not_comps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m         state[COMPONENT_LEARNER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_results(results)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:446\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    441\u001b[0m     func, remote_actor_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_func_and_remote_actor_id_by_state(\n\u001b[1;32m    442\u001b[0m         func, remote_actor_ids\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Send out remote requests.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_actors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[1;32m    452\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_result(\n\u001b[1;32m    453\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    454\u001b[0m     remote_calls\u001b[38;5;241m=\u001b[39mremote_calls,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     mark_healthy\u001b[38;5;241m=\u001b[39mmark_healthy,\n\u001b[1;32m    459\u001b[0m )\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:726\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._call_actors\u001b[0;34m(self, func, remote_actor_ids)\u001b[0m\n\u001b[1;32m    722\u001b[0m     calls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actors[i]\u001b[38;5;241m.\u001b[39mapply\u001b[38;5;241m.\u001b[39mremote(f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(remote_actor_ids, func)\n\u001b[1;32m    724\u001b[0m     ]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m     calls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actors[i]\u001b[38;5;241m.\u001b[39mapply\u001b[38;5;241m.\u001b[39mremote(func) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m remote_actor_ids]\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m calls\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:726\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m     calls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actors[i]\u001b[38;5;241m.\u001b[39mapply\u001b[38;5;241m.\u001b[39mremote(f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(remote_actor_ids, func)\n\u001b[1;32m    724\u001b[0m     ]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m     calls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m remote_actor_ids]\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m calls\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/actor.py:202\u001b[0m, in \u001b[0;36mActorMethod.remote\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mremote\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py:426\u001b[0m, in \u001b[0;36m_tracing_actor_method_invocation.<locals>._start_span\u001b[0;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m class_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_ref()\u001b[38;5;241m.\u001b[39m_ray_actor_creation_function_descriptor\u001b[38;5;241m.\u001b[39mclass_name\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    431\u001b[0m method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method_name\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/actor.py:345\u001b[0m, in \u001b[0;36mActorMethod._remote\u001b[0;34m(self, args, kwargs, name, num_returns, max_task_retries, retry_exceptions, concurrency_group, _generator_backpressure_num_objects, enable_task_events)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorator(invocation)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/actor.py:326\u001b[0m, in \u001b[0;36mActorMethod._remote.<locals>.invocation\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLost reference to actor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_method_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_method_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_task_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_task_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_backpressure_num_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_generator_backpressure_num_objects\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_task_events\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_task_events\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/actor.py:1481\u001b[0m, in \u001b[0;36mActorHandle._actor_method_call\u001b[0;34m(self, method_name, args, kwargs, name, num_returns, max_task_retries, retry_exceptions, concurrency_group_name, generator_backpressure_num_objects, enable_task_events)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generator_backpressure_num_objects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1479\u001b[0m     generator_backpressure_num_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1481\u001b[0m object_refs \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_actor_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_actor_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_actor_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_descriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlist_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_task_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exception_allowlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_actor_method_cpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_group_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrency_group_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_backpressure_num_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_task_events\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_returns \u001b[38;5;241m==\u001b[39m STREAMING_GENERATOR_RETURN:\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;66;03m# Streaming generator will return a single ref\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;66;03m# that is for the generator task.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(object_refs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:4280\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.submit_actor_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:4336\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.submit_actor_task\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Failed to submit task to actor ActorID(268fc8295ccff32ebaa8c67801000000) due to b\"Can't find actor 268fc8295ccff32ebaa8c67801000000. It might be dead or it's from a different cluster\""
     ]
    }
   ],
   "source": [
    "save_result = dqn.save()\n",
    "path_to_checkpoint = save_result.checkpoint.path\n",
    "print(\n",
    "    \"An Algorithm checkpoint has been created inside directory: \"\n",
    "    f\"'{path_to_checkpoint}'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring the saved model and executing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from ray.rllib.core.rl_module import RLModule\n",
    "import pathlib\n",
    "import torch\n",
    "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
    "from ray.rllib.core.columns import Columns\n",
    "best_checkpoint = \"/tmp/tmp_n7lvhjk\"#/tmp/tmpnj4amrtt\"\n",
    "\n",
    "# Create only the neural network (RLModule) from our checkpoint.\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    pathlib.Path(best_checkpoint) / \"learner_group\" / \"learner\" / \"rl_module\"\n",
    ")[\"default_policy\"]\n",
    "\n",
    "import PIL.Image\n",
    "env = custom_env_creator({})\n",
    "obs, info = env.reset()\n",
    "#env.render()\n",
    "\n",
    "for _ in range(120):\n",
    "\n",
    "    fwd_ins = {\"obs\": torch.Tensor([obs])}\n",
    "    fwd_outputs = rl_module.forward_inference(fwd_ins)\n",
    "    # this can be either deterministic or stochastic distribution\n",
    "    #action_dist = action_dist_class.from_logits(\n",
    "    #    fwd_outputs[\"action_dist_inputs\"]\n",
    "    #)\n",
    "    logits = convert_to_numpy(fwd_outputs[Columns.ACTION_DIST_INPUTS])\n",
    "    # Compute the next action from a batch (B=1) of observations.\n",
    "    ##torch_obs_batch = torch.Tensor(np.array([obs]))\n",
    "    #aforwInf = rl_module.forward_inference({\"obs\": torch_obs_batch})\n",
    "    ##action_logits = rl_module.forward_inference({\"obs\": torch_obs_batch})[\n",
    "    ##    \"actions\"\n",
    "    ##]\n",
    "    # The default RLModule used here produces action logits (from which\n",
    "    # we'll have to sample an action or use the max-likelihood one).\n",
    "    #action = torch.argmax(logits[0]).numpy()\n",
    "    #print(softmax(logits[0]))\n",
    "    action = np.random.choice(env.action_space.n, p=softmax(logits[0]))\n",
    "    print(softmax(logits[0]))\n",
    "    #action = dqn.compute_single_action(obs)\n",
    "    #action = env.action_space.sample()\n",
    "    print(logits)\n",
    "    print(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "\n",
    "    if terminated:\n",
    "        obs, info = env.reset()\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
