{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a custom Pong Environment\n",
    "\n",
    "Skipping initial frames to start without initial idle frames and Cutting episodes after 200 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "\n",
    "class CustomPongEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        super().__init__()\n",
    "        self.default_env = gym.make(\"ALE/Pong-v5\")\n",
    "\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 3), dtype=np.uint8)\n",
    "        self.action_space = self.default_env.action_space  # Example: move up or down\n",
    "        self.max_episode_steps = 200  # Set a max length for each episode to around 20 seconds of playtime\n",
    "        self.current_step = 0\n",
    "        self.skip_initial_steps = 40 \n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_step = 0\n",
    "        obs, info = self.default_env.reset()\n",
    "            # Skip the first `skip_initial_steps` by taking no-op actions\n",
    "        for _ in range(self.skip_initial_steps):\n",
    "            obs, _, terminated, truncated, _ = self.default_env.step(0)  # Assuming action `0` is no-op\n",
    "            if terminated or truncated:  # If the episode ends during skipping, reset again\n",
    "                obs, _ = self.default_env.reset(**kwargs)\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = self.default_env.step(action)\n",
    "        done = terminated or truncated or (self.current_step >= self.max_episode_steps)\n",
    "        return obs, reward, done, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show a random agent playing pong\n",
    "evaluate the episodes make sense and the agent acts while not getting stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode terminated\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import shimmy\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "\n",
    "import PIL.Image\n",
    "env_name = \"ALE/Pong-v5\"\n",
    "\n",
    "print(gym.pprint_registry())\n",
    "env = CustomPongEnv()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(observation.shape)\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "    if terminated:\n",
    "        observation, info = env.reset()\n",
    "        print(\"episode terminated\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a wrapper for the observation\n",
    "* convert to grayscale\n",
    "* normalize observation\n",
    "* convert data shape\n",
    "* removing top and bottom which is not part of the playing field\n",
    "* down sample the image for performance\n",
    "* adding padding around the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:07:43,194\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-02-26 19:07:45,275\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models.preprocessors import Preprocessor\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class MyWrapper(gym.ObservationWrapper):\n",
    "    dim = (210,160)\n",
    "    padding = 2\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Adjust the observation space to reflect the grayscale image shape\n",
    "        obs_shape = self.observation_space.shape\n",
    "        print(type(env.observation_space))\n",
    "        env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(84, 84, 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        print(type(self.observation_space))\n",
    "\n",
    "    def observation(self, incomming_observation):\n",
    "        observation = np.copy(incomming_observation)\n",
    "        # Convert the observation to grayscale\n",
    "        observation = np.dot(observation[...,:3], [0.333, 0.333, 0.333])\n",
    "        # Normalize the grayscale image to the range [0, 1]\n",
    "        observation = observation / 255.0\n",
    "        observation = observation[..., np.newaxis]\n",
    "        # Remove all but playingfield\n",
    "        observation = observation[34:-16]\n",
    "        # Reduce size of the image for faster inference\n",
    "        observation = observation[::2, ::2]\n",
    "        observation = np.pad(observation, ((self.padding, self.padding), (self.padding, self.padding), (0, 0)),mode='constant', constant_values=0) \n",
    "        return observation\n",
    "# Register the preprocessor\n",
    "#ModelCatalog.register_custom_preprocessor(\"custom_obs_preprocessor\", CustomObservationPreprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for debugging purposes\n",
    "visualization of a episode in mp4 video format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_observations_as_video(observations, output_path, fps=30):\n",
    "    \"\"\"\n",
    "    Saves a sequence of observations (frames) as a video file, with numerical array overlays.\n",
    "\n",
    "    Args:\n",
    "        observations (torch.Tensor or list of torch.Tensor): Tensor of frames with shape [batch_size, H, W, C]\n",
    "                                                             or a list of tensors each shaped [1, H, W, 1].\n",
    "        output_path (str): Path to save the output video file.\n",
    "        fps (int): Frames per second for the output video.\n",
    "    \"\"\"\n",
    "    # Handle PyTorch tensor input\n",
    "    if isinstance(observations, torch.Tensor):\n",
    "        # Ensure observations have shape [batch_size, H, W, C]\n",
    "        if len(observations.shape) == 4:  # [batch_size, H, W, C]\n",
    "            observations = [obs.squeeze().cpu().numpy() for obs in observations]\n",
    "        else:\n",
    "            raise ValueError(\"Expected observations tensor with shape [batch_size, H, W, C].\")\n",
    "\n",
    "    # Ensure observations is not empty\n",
    "    if observations is None or len(observations) == 0:\n",
    "        print(\"No observations to save.\")\n",
    "        return\n",
    "\n",
    "    # Process each observation\n",
    "    processed_frames = []\n",
    "    for obs in observations:\n",
    "        frame_rgb = None\n",
    "        # If observation is grayscale, convert to RGB\n",
    "        if len(obs.shape) == 2:  # Shape: [H, W]\n",
    "            frame_rgb = cv2.cvtColor((obs * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        elif len(obs.shape) == 3 and obs.shape[-1] == 1:  # Shape: [H, W, 1]\n",
    "            frame_rgb = cv2.cvtColor((obs.squeeze(-1) * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        elif len(obs.shape) == 3 and obs.shape[-1] == 3:  # Shape: [H, W, 3]\n",
    "            frame_rgb = (obs * 255).astype(np.uint8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected observation shape: {obs.shape}\")\n",
    "\n",
    "        # Overlay numerical values as text on the frame\n",
    "        overlay_frame = frame_rgb.copy()\n",
    "        height, width = frame_rgb.shape[:2]\n",
    "\n",
    "        processed_frames.append(overlay_frame)\n",
    "\n",
    "    # Get video dimensions\n",
    "    height, width, _ = processed_frames[0].shape\n",
    "\n",
    "    # Define the codec and initialize VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in processed_frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Distribution for Epsilon greedy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ray.rllib.models.distributions import Distribution\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.rllib.models.torch.torch_distributions import (TorchDistribution, TorchDeterministic)\n",
    "\n",
    "class CustomDistribution(TorchDistribution):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        logits: torch.Tensor = None,\n",
    "        probs: torch.Tensor = None,\n",
    "    ) -> None:\n",
    "        # We assert this here because to_deterministic makes this assumption.\n",
    "        assert (probs is None) != (\n",
    "            logits is None\n",
    "        ), \"Exactly one out of `probs` and `logits` must be set!\"\n",
    "\n",
    "        self.probs = probs\n",
    "        self.logits = logits\n",
    "        super().__init__(logits=logits, probs=probs)\n",
    "\n",
    "        # Build this distribution only if really needed (in `self.rsample()`). It's\n",
    "        # quite expensive according to cProfile.\n",
    "        self._one_hot = None\n",
    "        \n",
    "        # Epsilon settings\n",
    "        self.epsilon = 1\n",
    "        self.final_epsilon = 0.01 # 1 % of actions shall remain random\n",
    "        self.epsilon_decay = 5e-5  # Adjust decay rate as needed -> 10k Steps before min randomeness is reached\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        \"\"\"\n",
    "        Apply epsilon-greedy sampling logic.\n",
    "        \"\"\"\n",
    "        # Use torch's Categorical to handle logits or probs\n",
    "        categorical_dist = self._get_torch_distribution(\n",
    "            logits=self.logits, probs=self.probs\n",
    "        )\n",
    "\n",
    "        self.epsilon = max(\n",
    "                self.final_epsilon,\n",
    "                self.epsilon - self.epsilon_decay,\n",
    "            )\n",
    "        \n",
    "        # Get greedy action (exploitation)\n",
    "        greedy_action = torch.argmax(self.logits if self.logits is not None else self.probs, dim=-1)\n",
    "\n",
    "        # Get random action (exploration)\n",
    "        random_action = torch.randint(0, self.logits.shape[-1], greedy_action.shape)\n",
    "\n",
    "        # Perform epsilon-greedy decision\n",
    "        exploration_mask = torch.rand(greedy_action.shape) < self.epsilon\n",
    "        final_action = torch.where(exploration_mask, random_action, greedy_action)\n",
    "\n",
    "        return final_action\n",
    "\n",
    "    def _get_torch_distribution(\n",
    "        self,\n",
    "        logits: torch.Tensor = None,\n",
    "        probs: torch.Tensor = None,\n",
    "    ) -> \"torch.distributions.Distribution\":\n",
    "        return torch.distributions.categorical.Categorical(logits=logits, probs=probs)\n",
    "\n",
    "    def required_input_dim(space: gym.Space, **kwargs) -> int:\n",
    "        assert isinstance(space, gym.spaces.Discrete)\n",
    "        return int(space.n)\n",
    "\n",
    "    def rsample(self, sample_shape=()):\n",
    "        return self.sample(sample_shape=sample_shape)\n",
    "\n",
    "    @classmethod\n",
    "    def from_logits(cls, logits, **kwargs) -> \"CustomDistribution\":\n",
    "        return CustomDistribution(logits=logits, **kwargs)\n",
    "\n",
    "    def to_deterministic(self) -> \"TorchDeterministic\":\n",
    "        if self.probs is not None:\n",
    "            probs_or_logits = self.probs\n",
    "        else:\n",
    "            probs_or_logits = self.logits\n",
    "\n",
    "        return CustomDistribution(loc=torch.argmax(probs_or_logits, dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a DQN Model\n",
    "* Implementation of an online model\n",
    "* Implemenation of a target network with update policy\n",
    "* Implementation of forward inference\n",
    "* returning logits for the current and next observation as well as logits for the training network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleConfig\n",
    "from typing import Any, Dict\n",
    "\n",
    "from typing import Any, Dict, Type\n",
    "from ray.rllib.models.torch.torch_distributions import TorchDistribution\n",
    "\n",
    "from torch import flatten\n",
    "import json\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class CustomAtariRLModule(TorchRLModule):\n",
    "    def setup(self):\n",
    "        # Extract observation and action space information from the config\n",
    "        obs_shape = self.observation_space.shape  # (1, 84, 84)\n",
    "        num_actions = self.action_space.n        # Number of discrete actions (e.g., 6)\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Compute the size of the flattened layer\n",
    "        flattened_size = 64 * 7 * 7  # Adjusted for (84, 84) input size\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.action_head = nn.Linear(512, num_actions)\n",
    "        \n",
    "        # Target network\n",
    "        self.target_conv1 = nn.Conv2d(1, 32, 8, stride=4)\n",
    "        self.target_conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.target_conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        self.target_fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.target_action_head = nn.Linear(512, self.action_space.n)\n",
    "\n",
    "        # Copy weights from the online network to the target network\n",
    "        self._update_target_network()\n",
    "\n",
    "        self.target_update_frequency = 400;\n",
    "        self.iter = 0;\n",
    "\n",
    "    def get_exploration_action_dist_cls(self) -> Type[TorchDistribution]:\n",
    "        return CustomDistribution\n",
    "\n",
    "    def _forward_train(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        # Forward pass through the network\n",
    "        #print(batch[\"obs\"].size())\n",
    "        self.iter  += 1\n",
    "\n",
    "        #update targaet network\n",
    "        if self.iter % self.target_update_frequency == 0:\n",
    "            self._update_target_network()\n",
    "        \n",
    "        action_logits = None\n",
    "\n",
    "        #if batch[\"obs\"].shape[0] > 1:\n",
    "        #    print(\"batchsize: \", batch[\"obs\"].shape[0])\n",
    "        #    save_observations_as_video(batch[\"obs\"], \"./test_obs_vid.mp4\")\n",
    "\n",
    "        x = batch[\"obs\"].permute(0, 3, 1, 2)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        #x = flatten(x).reshape(1,64*7*7)\n",
    "        x = x.reshape(x.size(0), -1) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_logits = self.action_head(x)\n",
    "\n",
    "\n",
    "        #print(\"actionLogits: \", action_logits)\n",
    "\n",
    "        next_q_values = None\n",
    "\n",
    "        # Compute Q-values for next states using the same logic (or via a target network)\n",
    "        if 'new_obs' in batch.keys():\n",
    "            #print(\"next obs available\")\n",
    "            next_obs = batch[\"new_obs\"].permute(0, 3, 1, 2)  # Handle next states\n",
    "            next_x = torch.relu(self.conv1(next_obs))\n",
    "            next_x = torch.relu(self.conv2(next_x))\n",
    "            next_x = torch.relu(self.conv3(next_x))\n",
    "            next_x = next_x.reshape(next_x.size(0), -1)\n",
    "            next_x = torch.relu(self.fc1(next_x))\n",
    "            next_q_values = self.action_head(next_x)   # Q-values for next observations\n",
    "\n",
    "\n",
    "            # Compute Q-values for next observations using the target network\n",
    "            with torch.no_grad():  # Ensure no gradients are computed for the target network\n",
    "                target_next_x = torch.relu(self.target_conv1(next_obs))\n",
    "                target_next_x = torch.relu(self.target_conv2(target_next_x))\n",
    "                target_next_x = torch.relu(self.target_conv3(target_next_x))\n",
    "                target_next_x = target_next_x.reshape(target_next_x.size(0), -1)\n",
    "                target_next_x = torch.relu(self.target_fc1(target_next_x))\n",
    "                qf_target_next_preds = self.target_action_head(target_next_x)\n",
    "                \n",
    "                #print(\"targetActionLogits: \", qf_target_next_preds)\n",
    "            return {\"action_dist_inputs\": action_logits, \"qf_preds\":action_logits, \"qf_target_next_preds\": qf_target_next_preds, \"qf_next_preds\":next_q_values} \n",
    "        # Return action distribution\n",
    "        return {\"action_dist_inputs\": action_logits, \"qf_preds\":action_logits}\n",
    "\n",
    "    def _forward_inference(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            return self._forward_train(batch, explore = False)\n",
    "\n",
    "    def _forward_exploration(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        #with torch.no_grad():\n",
    "         return self._forward_train(batch)\n",
    "\n",
    "\n",
    "    def _update_target_network(self):\n",
    "        for target_param, param in zip(self.target_parameters(), self.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        #print(list(self.conv1.parameters()))\n",
    "\n",
    "    def target_parameters(self):\n",
    "    # Return all target network parameters\n",
    "        return list(self.target_conv1.parameters()) + \\\n",
    "               list(self.target_conv2.parameters()) + \\\n",
    "               list(self.target_conv3.parameters()) + \\\n",
    "               list(self.target_fc1.parameters()) + \\\n",
    "               list(self.target_action_head.parameters())\n",
    "\n",
    "    def parameters(self):\n",
    "        # Return all online network parameters\n",
    "        return list(self.conv1.parameters()) + \\\n",
    "               list(self.conv2.parameters()) + \\\n",
    "               list(self.conv3.parameters()) + \\\n",
    "               list(self.fc1.parameters()) + \\\n",
    "               list(self.action_head.parameters())\n",
    "\n",
    "# Register the custom RLModule\n",
    "ModelCatalog.register_custom_model(\"custom_atari_module\", CustomAtariRLModule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DQN Agent of Rllib with needed Settings and debug callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:07:46,402\tWARNING deprecation.py:50 -- DeprecationWarning: `rollouts` has been deprecated. Use `AlgorithmConfig.env_runners(..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "{'type': 'PrioritizedEpisodeReplayBuffer', 'capacity': 2048, 'alpha': 0.5, 'beta': 0.5, 'replay_sequence_length': 200}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "import ale_py\n",
    "import shimmy\n",
    "from gymnasium.envs.registration import register\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "from ray.rllib.env.single_agent_env_runner import SingleAgentEnvRunner\n",
    "\n",
    "\n",
    "\n",
    "# Wrap the environment\n",
    "def custom_env_creator(env_config):\n",
    "    env = CustomPongEnv() #gym.make(\"ALE/Pong-v5\",  **env_config.get(\"gym_kwargs\", {}))\n",
    "    print(env.observation_space, env.action_space)\n",
    "    env = MyWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "class DebugCallback(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, episode, **kwargs):\n",
    "        print(f\"Episode started.\")\n",
    "    \n",
    "    def on_episode_end(self, *, episode, **kwargs):\n",
    "        print(f\"Episode ended with length {episode.__len__()}.\")\n",
    "    \n",
    "    def on_sample_end(self, *, samples, **kwargs):\n",
    "        print(\"Sample batch collected:\")\n",
    "    \n",
    "        # If the samples are in SAEps format (list of episodes):\n",
    "        if isinstance(samples, list):\n",
    "            for idx, episode in enumerate(samples):\n",
    "                save_observations_as_video(episode.get_observations(), \"./samles.mp4\")\n",
    "                print(f\"  Episode {idx}:\")\n",
    "                print(f\"    Length: {episode.__len__()}\")  # Total length of the episode\n",
    "                print(f\"    Done: {episode.is_done}\")  # Done flag of the episode\n",
    "                print(f\"    Reward: {episode.get_return}\")  # Episode reward\n",
    "                \n",
    "                # Access observations, rewards, and done flags\n",
    "                # Assuming `episode` stores these as numpy arrays or lists\n",
    "                print(f\"    Observations (sample size): {len(episode.get_observations())}\")  # Show first observation in the episode\n",
    "                #print(f\"    Rewards (sample): {episode.get_rewards[:5]}\")  # Show first 5 rewards in the episode\n",
    "                #print(f\"    Done flags (sample): {episode.dones[:5]}\")  # Show first 5 done flags in the episode\n",
    "    \n",
    "        else:\n",
    "            print(f\"  Unexpected format: {samples}\")\n",
    "\n",
    "        #save_observations_as_video(samples.get_observations(), \"./samles.mp4\")\n",
    "\n",
    "    def before_learn_on_batch(self, *, policy:Policy, train_batch:SampleBatch, result, **kwargs):\n",
    "        print(\"====== Training Batch Debug ======\")\n",
    "        print(f\"Train Batch Size: {train_batch.count}\")\n",
    "\n",
    "        if SampleBatch.SEQ_LENS in train_batch:\n",
    "            print(f\"Sequence Lengths: {train_batch[SampleBatch.SEQ_LENS]}\")\n",
    "        \n",
    "        print(\"Sampled Observations:\")\n",
    "        save_observations_as_video(train_batch[\"obs\"], \"./train_batch.mp4\")\n",
    "        \n",
    "        raise Exception(\"blub\")\n",
    "\n",
    "environment = custom_env_creator({})\n",
    "\n",
    "# Configure the DQN Algorithm\n",
    "config:DQNConfig = (\n",
    "    DQNConfig()\n",
    "    #.callbacks(DebugCallback)\n",
    "    .environment(\n",
    "        env=\"custom_pong_env\",  # Atari Pong environment (gymnasium)\n",
    "        env_config={\"episodic\": True},\n",
    "        #env_config={\n",
    "        #    \"preprocessor_pref\": \"custom_obs_preprocessor\",\n",
    "        #},\n",
    "    )\n",
    "    #.reporting(min_train_timesteps_per_iteration=20,\n",
    "    #           min_time_s_per_iteration=0,\n",
    "    #           min_sample_timesteps_per_iteration = 0,)\n",
    "    .framework(\"torch\")     # Use PyTorch (preferred for modern RL)\n",
    "    .env_runners(num_env_runners=1,\n",
    "        num_gpus_per_env_runner=1,\n",
    "        batch_mode='truncate_episodes') \n",
    "    .rollouts(rollout_fragment_length=200,\n",
    "              batch_mode='truncate_episodes')\n",
    "    .training(\n",
    "        replay_buffer_config={\n",
    "                \"type\": \"PrioritizedEpisodeReplayBuffer\",\n",
    "                \"capacity\": 2048,\n",
    "                \"alpha\": 0.5,\n",
    "                \"beta\": 0.5,\n",
    "                \"replay_sequence_length\":200\n",
    "            },\n",
    "        train_batch_size=200,\n",
    "        gamma=0.50,            # Discount factor\n",
    "        lr=1e-4,               # Learning rate\n",
    "        target_network_update_freq=200,  # Update target network frequency\n",
    "        num_steps_sampled_before_learning_starts = 1000,\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=RLModuleSpec(module_class=CustomAtariRLModule)\n",
    "    )\n",
    "    .resources(num_gpus=1)  # Set this to 1 if you have a GPU\n",
    "    .api_stack(enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,)\n",
    "    .debugging(logger_config={\"log_level\": \"DEBUG\"})\n",
    "    .evaluation(\n",
    "        evaluation_interval=5,\n",
    "        evaluation_config={\"explore\": False}\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=1,\n",
    "        num_gpus_per_learner=1,\n",
    "    )\n",
    "    #.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)\n",
    ")\n",
    "\n",
    "\n",
    "print(config[\"replay_buffer_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Resource Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Number of GPUs: 1\n",
      "CUDA current device: 0\n",
      "CUDA device name: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RAY_record_ref_creation_sites=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:06:47,348\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-02-26 19:06:48,114\tWARNING dqn.py:418 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      ":job_id:01000000\n",
      ":actor_name:SingleAgentEnvRunner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:569: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      ":job_id:01000000\n",
      ":actor_name:SingleAgentEnvRunner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:06:48,619\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "2025-02-26 19:06:48,969\tWARNING dqn.py:418 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:06:49,259\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (84, 84, 3), uint8) Discrete(6)\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:_WrappedExecutable\n",
      "2025-02-26 19:06:50,497\tINFO config.py:83 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:_WrappedExecutable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:06:50,638\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2025-02-26 19:06:51,075\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/docker/machinelearning/torch/data/uni/reinforcementLerning/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "update from episode\n",
      "Iteration 0 results:\n",
      "date: 2025-02-26_19-06-56\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    default_agent: -5.666666666666667\n",
      "  episode_duration_sec_mean: 0.8057750501980384\n",
      "  episode_len_max: 200\n",
      "  episode_len_mean: 200.0\n",
      "  episode_len_min: 200\n",
      "  episode_return_max: -5.0\n",
      "  episode_return_mean: -5.666666666666667\n",
      "  episode_return_min: -6.0\n",
      "  module_episode_returns_mean:\n",
      "    default_policy: -5.666666666666667\n",
      "  num_agent_steps_sampled:\n",
      "    default_agent: 1000\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    default_agent: 1000\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_sampled_lifetime: 1000\n",
      "  num_episodes: 5\n",
      "  num_episodes_lifetime: 5\n",
      "  num_module_steps_sampled:\n",
      "    default_policy: 1000\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    default_policy: 1000\n",
      "  sample: 0.8004224785063841\n",
      "  time_between_sampling: 0.014255260814421003\n",
      "  weights_seq_no: 0.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 1\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: server-0815\n",
      "iterations_since_restore: 1\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector_timer: 0.02678914275020361\n",
      "    num_env_steps_trained: 200\n",
      "    num_env_steps_trained_lifetime: 200\n",
      "    num_module_steps_trained: 200\n",
      "    num_module_steps_trained_lifetime: 200\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 1681062\n",
      "  default_policy:\n",
      "    default_optimizer_learning_rate: 0.0001\n",
      "    gradients_default_optimizer_global_norm: 0.03453899919986725\n",
      "    module_train_batch_size_mean: 200\n",
      "    num_module_steps_trained: 200\n",
      "    num_module_steps_trained_lifetime: 200\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 1681062\n",
      "    qf_loss: 0.020499765872955322\n",
      "    qf_max: 0.02454354427754879\n",
      "    qf_mean: 0.00044436968164518476\n",
      "    qf_min: -0.04587114229798317\n",
      "    td_error_mean: 0.058480117470026016\n",
      "    total_loss: 0.020499765872955322\n",
      "    weights_seq_no: 1.0\n",
      "node_ip: 192.168.178.28\n",
      "num_env_steps_sampled_lifetime: 1000\n",
      "num_training_step_calls_per_iteration: 5\n",
      "perf:\n",
      "  cpu_util_percent: 38.4875\n",
      "  ram_util_percent: 18.525\n",
      "pid: 1849146\n",
      "time_since_restore: 5.251729726791382\n",
      "time_this_iter_s: 5.251729726791382\n",
      "time_total_s: 5.251729726791382\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.8117303022779516\n",
      "  learner_update_timer: 0.7599841142073274\n",
      "  replay_buffer_add_data_timer: 0.004928513834563499\n",
      "  replay_buffer_sampling_timer: 0.05217511439695954\n",
      "  replay_buffer_update_prios_timer: 0.0025419113226234913\n",
      "  restore_workers: 2.8241278918427413e-05\n",
      "  synch_weights: 0.11105289263650775\n",
      "  training_iteration: 5.244220495223999\n",
      "  training_step: 0.8265027645125286\n",
      "timestamp: 1740596816\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/tmp/tmperiuxo2v), metrics={'timers': {'training_iteration': 5.244220495223999, 'restore_workers': 2.8241278918427413e-05, 'training_step': 0.8265027645125286, 'env_runner_sampling_timer': 0.8117303022779516, 'replay_buffer_add_data_timer': 0.004928513834563499, 'replay_buffer_sampling_timer': 0.05217511439695954, 'learner_update_timer': 0.7599841142073274, 'replay_buffer_update_prios_timer': 0.0025419113226234913, 'synch_weights': 0.11105289263650775}, 'env_runners': {'sample': 0.8004224785063841, 'episode_len_max': 200, 'num_module_steps_sampled': {'default_policy': 1000}, 'episode_len_mean': 200.0, 'num_agent_steps_sampled_lifetime': {'default_agent': 1000}, 'num_episodes_lifetime': 5, 'num_agent_steps_sampled': {'default_agent': 1000}, 'num_module_steps_sampled_lifetime': {'default_policy': 1000}, 'agent_episode_returns_mean': {'default_agent': -5.666666666666667}, 'num_env_steps_sampled_lifetime': 1000, 'module_episode_returns_mean': {'default_policy': -5.666666666666667}, 'episode_return_mean': -5.666666666666667, 'episode_len_min': 200, 'episode_return_max': -5.0, 'episode_duration_sec_mean': 0.8057750501980384, 'num_episodes': 5, 'episode_return_min': -6.0, 'num_env_steps_sampled': 1000, 'weights_seq_no': 0.0, 'time_between_sampling': 0.014255260814421003}, 'num_training_step_calls_per_iteration': 5, 'learners': {'__all_modules__': {'num_env_steps_trained_lifetime': 200, 'num_module_steps_trained_lifetime': 200, 'num_env_steps_trained': 200, 'num_module_steps_trained': 200, 'num_trainable_parameters': 1681062, 'learner_connector_timer': 0.02678914275020361, 'num_non_trainable_parameters': 0}, 'default_policy': {'qf_max': 0.02454354427754879, 'weights_seq_no': 1.0, 'num_module_steps_trained': 200, 'num_trainable_parameters': 1681062, 'total_loss': 0.020499765872955322, 'td_error_mean': 0.058480117470026016, 'qf_loss': 0.020499765872955322, 'module_train_batch_size_mean': 200, 'num_non_trainable_parameters': 0, 'qf_mean': 0.00044436968164518476, 'default_optimizer_learning_rate': 0.0001, 'gradients_default_optimizer_global_norm': 0.03453899919986725, 'num_module_steps_trained_lifetime': 200, 'qf_min': -0.04587114229798317}}, 'num_env_steps_sampled_lifetime': 1000, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-02-26_19-06-56', 'timestamp': 1740596816, 'time_this_iter_s': 5.251729726791382, 'time_total_s': 5.251729726791382, 'pid': 1849146, 'hostname': 'server-0815', 'node_ip': '192.168.178.28', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'custom_pong_env', 'env_config': {'episodic': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 1, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 1, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.5, 'lr': 0.0001, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 200, 'num_epochs': 1, 'minibatch_size': None, 'shuffle_batch_per_epoch': False, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f15c6bd3c70>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 5, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': {'log_level': 'DEBUG'}, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': RLModuleSpec(module_class=<class '__main__.CustomAtariRLModule'>, observation_space=None, action_space=None, inference_only=False, learner_only=False, model_config=None, catalog_class=None, load_state_path=None, model_config_dict=None), 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'epsilon': [(0, 1.0), (10000, 0.05)], 'target_network_update_freq': 200, 'num_steps_sampled_before_learning_starts': 1000, 'store_buffer_in_checkpoints': False, 'adam_epsilon': 1e-08, 'tau': 1.0, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'td_error_loss_fn': 'huber', 'categorical_distribution_temperature': 1.0, 'replay_buffer_config': {'type': 'PrioritizedEpisodeReplayBuffer', 'capacity': 2048, 'alpha': 0.5, 'beta': 0.5, 'replay_sequence_length': 200}, 'lr_schedule': None, 'class': <class 'ray.rllib.algorithms.dqn.dqn.DQNConfig'>, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 5.251729726791382, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(38.4875), 'ram_util_percent': np.float64(18.525)}})\n",
      "update from episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:06:59,451\tERROR actor_manager.py:804 -- Ray error (\u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1849146, ip=192.168.178.28, actor_id=fc541f322d515afef403526e01000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f17385e4730>)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.), taking actor 1 out of service.\n",
      "2025-02-26 19:06:59,452\tERROR actor_manager.py:635 -- \u001b[36mray::SingleAgentEnvRunner.apply()\u001b[39m (pid=1849146, ip=192.168.178.28, actor_id=fc541f322d515afef403526e01000000, repr=<ray.rllib.env.single_agent_env_runner._modify_class.<locals>.Class object at 0x7f17385e4730>)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(ffffffffffffffffffffffffffffffffffffffff01000000) was cancelled.\n",
      "NoneType: None\n",
      "2025-02-26 19:06:59,459\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update from episode\n",
      "update from episode\n",
      "update from episode\n",
      "update from episode\n",
      "update from episode\n",
      "update from episode\n",
      "update from episode\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.utils.replay_buffers import PrioritizedEpisodeReplayBuffer\n",
    "\n",
    "%env RAY_record_ref_creation_sites=1\n",
    "\n",
    "ray.shutdown()  # Cleanly shutdown the current Ray session\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True, local_mode=True, object_store_memory=1e10, num_cpus=28)#num_gpus=1, logging_level=\"debug\"\n",
    "print(ray.get_gpu_ids())\n",
    "\n",
    "tune.register_env(\"custom_pong_env\", custom_env_creator)\n",
    "\n",
    "\n",
    "# Build the DQN Algorithm\n",
    "dqn: Algorithm = config.build()\n",
    "#dqn.restore_from_path(\"/tmp/tmpm9j1s9hd\")\n",
    "\n",
    "# Train the agent\n",
    "print(\"Starting training...\")\n",
    "for i in range(301):  # Increase the number of iterations for better performance\n",
    "    result = dqn.train()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i} results:\")\n",
    "        print(pretty_print(result))\n",
    "        #Save checkpoints periodically\n",
    "    \n",
    "        checkpoint = dqn.save()\n",
    "        print(f\"Checkpoint saved at {checkpoint}\")\n",
    "\n",
    "# Shut down Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the model manually if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result = dqn.save()\n",
    "path_to_checkpoint = save_result.checkpoint.path\n",
    "print(\n",
    "    \"An Algorithm checkpoint has been created inside directory: \"\n",
    "    f\"'{path_to_checkpoint}'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring the saved model and executing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from ray.rllib.core.rl_module import RLModule\n",
    "import pathlib\n",
    "import torch\n",
    "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
    "from ray.rllib.core.columns import Columns\n",
    "best_checkpoint = \"/tmp/tmp_n7lvhjk\"#/tmp/tmpnj4amrtt\"\n",
    "\n",
    "# Create only the neural network (RLModule) from our checkpoint.\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    pathlib.Path(best_checkpoint) / \"learner_group\" / \"learner\" / \"rl_module\"\n",
    ")[\"default_policy\"]\n",
    "\n",
    "import PIL.Image\n",
    "env = custom_env_creator({})\n",
    "obs, info = env.reset()\n",
    "#env.render()\n",
    "\n",
    "for _ in range(120):\n",
    "\n",
    "    fwd_ins = {\"obs\": torch.Tensor([obs])}\n",
    "    fwd_outputs = rl_module.forward_inference(fwd_ins)\n",
    "    # this can be either deterministic or stochastic distribution\n",
    "    #action_dist = action_dist_class.from_logits(\n",
    "    #    fwd_outputs[\"action_dist_inputs\"]\n",
    "    #)\n",
    "    logits = convert_to_numpy(fwd_outputs[Columns.ACTION_DIST_INPUTS])\n",
    "    # Compute the next action from a batch (B=1) of observations.\n",
    "    ##torch_obs_batch = torch.Tensor(np.array([obs]))\n",
    "    #aforwInf = rl_module.forward_inference({\"obs\": torch_obs_batch})\n",
    "    ##action_logits = rl_module.forward_inference({\"obs\": torch_obs_batch})[\n",
    "    ##    \"actions\"\n",
    "    ##]\n",
    "    # The default RLModule used here produces action logits (from which\n",
    "    # we'll have to sample an action or use the max-likelihood one).\n",
    "    #action = torch.argmax(logits[0]).numpy()\n",
    "    #print(softmax(logits[0]))\n",
    "    action = np.random.choice(env.action_space.n, p=softmax(logits[0]))\n",
    "    print(softmax(logits[0]))\n",
    "    #action = dqn.compute_single_action(obs)\n",
    "    #action = env.action_space.sample()\n",
    "    print(logits)\n",
    "    print(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(obs)\n",
    "    plt.show()\n",
    "\n",
    "    if terminated:\n",
    "        obs, info = env.reset()\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
